% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/constrainsReg.R
\name{classo}
\alias{classo}
\title{Fits regularization paths for lasso penalized learning problems with linear constraints.}
\usage{
classo(y, Z, Zc = NULL, intercept = TRUE, pf = rep(1, times = p),
  lam = NULL, nlam = 100, lambda.factor = ifelse(n < p, 0.05, 0.001),
  dfmax = p, pfmax = min(dfmax * 1.5, p), u = 1, mu_ratio = 1.01,
  tol = 1e-10, outer_maxiter = 3e+08, outer_eps = 1e-08,
  inner_maxiter = 1e+06, inner_eps = 1e-08, A = rep(1, times = p),
  b = 0, beta.ini)
}
\arguments{
\item{y}{a vector of response variable with length n.}

\item{Z}{a \eqn{n*p} matrix after taking log transformation on compositional data.}

\item{Zc}{a design matrix of other covariates considered. Default is \code{NULL}.}

\item{intercept}{Whether to include intercept in the model. Default is TRUE.}

\item{pf}{penalty factor, a vector in length of p. Separate penalty weights can be applied to each coefficience
\eqn{\beta} for composition variates to allow differential shrinkage. Can be 0 for some \eqn{\beta}'s,
which implies no shrinkage, and results in that composition always being included in the model.
Default value for each entry is the 1.}

\item{lam}{a user supplied lambda sequence. Typically, by leaving this option unspecified users can have the
program compute its own \code{lam} sequence based on \code{nlam} and \code{lambda.factor}.
Supplying a value of lambda overrides this.
If \code{lam} is provided but a scaler and \code{nlam}\eqn{>}1,
\code{lam} sequence is also created starting from \code{lam}.
If a sequence of lambda is provided, it is better to supply a decreasing one,
if not, the program will sort user-defined \code{lambda} sequence in decreasing order
automatically.}

\item{nlam}{the length of \code{lam} sequence. Default is 100.}

\item{lambda.factor}{the factor for getting the minimal lambda in \code{lam} sequence, where
\code{min(lam)} = \code{lambda.factor} * \code{max(lam)}.
\code{max(lam)} is the smallest value of \code{lam} for which all penalized group are zero's.
The default depends on the relationship between \eqn{n}
and \eqn{p1}
(the number of predictors to be penalized).
If \eqn{n >= p1}, the default is \code{0.001}, close to zero.
If \eqn{n < p1}, the default is \code{0.05}. A very small value of
\code{lambda.factor}
will lead to a saturated fit. It takes no effect if there is user-defined lambda sequence.}

\item{dfmax}{limit the maximum number of groups in the model. Useful for very large \eqn{p},
if a partial path is desired. Default is \eqn{p}.}

\item{pfmax}{limit the maximum number of groups ever to be nonzero. For example once a group enters the
model along the path, no matter how many times it exits or re-enters model through the path,
it will be counted only once. Default is \code{min(dfmax*1.5, p)}.}

\item{u}{\code{u} is the inital value for penalty parameter of augmented Lanrange method adopted in
outer loop - default value is 1.}

\item{mu_ratio}{\code{mu_ratio} is the increasing ratio for \code{u}. Default value is 1.01.
Inital values for scaled Lagrange multipliers are set as 0's.
If \code{mu_ratio} < 1,
there is no linear constraints included. Group lasso coefficients are estimated.}

\item{tol}{tolerance for vectors betas to be considered as none zero's. For example, coefficient
\eqn{\beta_j} for group j, if \eqn{max(abs(\beta_j))} < \code{tol}, set \eqn{\beta_j} as 0's.
Default value is 0.}

\item{outer_maxiter}{\code{outer_maxiter} is the maximun munber of loops allowed for Augmented Lanrange method;
and \code{outer_eps} is the convergence termination tolerance.}

\item{outer_eps}{\code{outer_maxiter} is the maximun munber of loops allowed for Augmented Lanrange method;
and \code{outer_eps} is the convergence termination tolerance.}

\item{inner_maxiter}{\code{inner_maxiter} is the maximun munber of loops allowed for blockwise-GMD;
and \code{inner_eps} is the convergence termination tolerance.}

\item{inner_eps}{\code{inner_maxiter} is the maximun munber of loops allowed for blockwise-GMD;
and \code{inner_eps} is the convergence termination tolerance.}

\item{A, b}{linear equality constraints \eqn{A\beta_p = b}, where \eqn{b} is a scaler,
and \eqn{A} is a vector with length \code{p}. Default values, \eqn{b} is a vector of 0 and
\code{A = rep(1, times = p)}.}

\item{beta.ini}{inital value of beta}
}
\description{
Fits regularization paths for linear constraints lasso penalized learning problems at a sequence of regularization parameters lambda.
}
